{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T06:25:48.821718Z",
     "start_time": "2019-06-28T06:25:46.556153Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from nltk.cluster.util import cosine_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GloVe Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia 2014 + Gigaword 5\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "Word\n",
    "embeddings are substantially successful in capturing semantic\n",
    "relations among words,\n",
    "The Euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare GloVe for word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T19:55:40.341887Z",
     "start_time": "2019-06-27T19:33:53.233168Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-27 20:33:53--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2019-06-27 20:33:53--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2019-06-27 20:33:54--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M   439KB/s    in 21m 11s \n",
      "\n",
      "2019-06-27 20:55:06 (662 KB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T06:21:46.518737Z",
     "start_time": "2019-06-28T06:21:46.512392Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_word_vector(vector_path):\n",
    "    word_embeddings = {}\n",
    "    with open(vector_path, \"r\", encoding='utf-8') as vector_file:\n",
    "        for line in vector_file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            word_embeddings[word] = coefs\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T06:23:01.107476Z",
     "start_time": "2019-06-28T06:22:06.971318Z"
    }
   },
   "outputs": [],
   "source": [
    "word_embeddings_100d = extract_word_vector(\"./glove.6B.100d.txt\")\n",
    "word_embeddings_300d = extract_word_vector(\"./glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GloVe word embeddings contains 400K different terms. \n",
    "GloVe contains 4 different dimension vectors - 50d, 100d, 200d & 300d.\n",
    "The difference relates to how much information from 400K terms are squeezed into each dimension vector.\n",
    "Vectors with lower dimensional space would hold more syntactic information \"a \"\n",
    "\n",
    "lower dimensional vectors such as 32 may contain primarily syntactic information, possibly because of the reduced dimensionality not having as much space to hold topical information. Vectors of 300 or 400 dimensions may have much more topical information. For a very crude description, in a very high dimensional space, basketball and hockey may have a relatively far cosine distance from each other, but in a lower dimensional space the distance may be closer together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T06:24:05.818532Z",
     "start_time": "2019-06-28T06:24:05.813078Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_sentence_vector(word_embeddings, sentence, we_dim):\n",
    "    vector = np.zeros((we_dim, ))\n",
    "\n",
    "    sentence_length = len(sentence) + 0.001\n",
    "\n",
    "    if sentence:\n",
    "        sentence_embeddings = sum([word_embeddings.get(word, vector) for word in sentence])\n",
    "\n",
    "        vector = sentence_embeddings/sentence_length\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T06:24:54.508826Z",
     "start_time": "2019-06-28T06:24:54.504453Z"
    }
   },
   "outputs": [],
   "source": [
    "def sentence_similarity(vector1, vector2):\n",
    "    similarity_score = 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "    if np.isnan(similarity_score):\n",
    "        similarity_score = 0\n",
    "\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T06:24:56.397528Z",
     "start_time": "2019-06-28T06:24:56.391194Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_similarity_matrix(sentences, sentence_vectors, we_dim):\n",
    "    sentence_length = len(sentences)\n",
    "    \n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((sentence_length, sentence_length))\n",
    "    \n",
    "    # create index of word pairs\n",
    "    permutation_set = list(itertools.permutations(range(0, sentence_length), 2))\n",
    "\n",
    "    for pair in permutation_set:\n",
    "        idx1, idx2 = pair\n",
    "        similarity_matrix[idx1][idx2] = sentence_similarity(sentence_vectors[idx1].reshape(we_dim), sentence_vectors[idx2].reshape(we_dim))\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T06:44:56.767452Z",
     "start_time": "2019-06-28T06:44:56.762800Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_glove_similarity_matrix(sample_sentences, word_embeddings, we_dim):\n",
    "    sentence_vectors = []\n",
    "\n",
    "    for review_sentence in sample_sentences:\n",
    "        sentence_vector = get_sentence_vector(word_embeddings, review_sentence, we_dim)\n",
    "        sentence_vectors.append(sentence_vector)\n",
    "    \n",
    "    glove_similarity_matrix = build_similarity_matrix(sample_sentences, sentence_vectors, we_dim)\n",
    "\n",
    "    return glove_similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T07:24:03.499736Z",
     "start_time": "2019-06-28T07:24:03.494933Z"
    }
   },
   "outputs": [],
   "source": [
    "def closest_sentence(sample_sentences, similarity_matrix):\n",
    "    print(similarity_matrix)\n",
    "    max_indx = np.argmax(similarity_matrix, axis=0)\n",
    "    return {\" \".join(sentence): \" \".join(sample_sentences[max_indx[e]]) for e, sentence in enumerate(sample_sentences)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sports VS Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T07:17:52.267969Z",
     "start_time": "2019-06-28T07:17:52.263780Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_sentences4 = [\"basketball\", \"taekwondo\", \"karate\", \"sushi\", \"burger\"]\n",
    "sample_sentences4 = [sentence.split(\" \") for sentence in sample_sentences4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T07:17:53.940765Z",
     "start_time": "2019-06-28T07:17:53.930614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.44787012  0.44513391  0.03073995  0.04361972]\n",
      " [ 0.44787012  0.          0.69275586  0.1619114  -0.07873317]\n",
      " [ 0.44513391  0.69275586  0.          0.33181058  0.1346807 ]\n",
      " [ 0.03073995  0.1619114   0.33181058  0.          0.48382605]\n",
      " [ 0.04361972 -0.07873317  0.1346807   0.48382605  0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'basketball': 'taekwondo',\n",
       " 'taekwondo': 'karate',\n",
       " 'karate': 'taekwondo',\n",
       " 'sushi': 'burger',\n",
       " 'burger': 'sushi'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmat_100d_4 = get_glove_similarity_matrix(sample_sentences4, word_embeddings_100d, 100)\n",
    "closest_sentence(sample_sentences4, gmat_100d_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T07:17:58.051816Z",
     "start_time": "2019-06-28T07:17:58.042381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.35349359  0.30225721 -0.04833672  0.03087713]\n",
      " [ 0.35349359  0.          0.61689786  0.05764558 -0.11130884]\n",
      " [ 0.30225721  0.61689786  0.          0.22528532  0.0337832 ]\n",
      " [-0.04833672  0.05764558  0.22528532  0.          0.39702389]\n",
      " [ 0.03087713 -0.11130884  0.0337832   0.39702389  0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'basketball': 'taekwondo',\n",
       " 'taekwondo': 'karate',\n",
       " 'karate': 'taekwondo',\n",
       " 'sushi': 'burger',\n",
       " 'burger': 'sushi'}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmat_300d_4 = get_glove_similarity_matrix(sample_sentences4, word_embeddings_300d, 300)\n",
    "closest_sentence(sample_sentences4, gmat_300d_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How close are these single words as concepts?\n",
    "Looking at the similarity matrix of both dimensional spaces, the closest word pairs fall between 0.3 to 0.6. \n",
    "\n",
    "`taekwondo` is the closest term to `basketball` (0.45) as they both are considered sports.\n",
    "\n",
    "However `taekwondo` is definitely closer to `karate` (0.69) because both of them are types of martial arts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T07:12:24.125101Z",
     "start_time": "2019-06-28T07:12:24.120846Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_sentences3 = [\"soccer\", \"tennis\", \"rugby\", \"badminton\"]\n",
    "sample_sentences3 = [sentence.split(\" \") for sentence in sample_sentences3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T07:12:25.591043Z",
     "start_time": "2019-06-28T07:12:25.579571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.703636   0.7242489  0.57408756]\n",
      " [0.703636   0.         0.56346737 0.77421505]\n",
      " [0.7242489  0.56346737 0.         0.53717555]\n",
      " [0.57408756 0.77421505 0.53717555 0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'soccer': 'rugby',\n",
       " 'tennis': 'badminton',\n",
       " 'rugby': 'soccer',\n",
       " 'badminton': 'tennis'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmat_100d_3 = get_glove_similarity_matrix(sample_sentences3, word_embeddings_100d, 100)\n",
    "closest_sentence(sample_sentences3, gmat_100d_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T07:26:46.058990Z",
     "start_time": "2019-06-28T07:26:46.050235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.5584686  0.53458385 0.43728225]\n",
      " [0.5584686  0.         0.44168142 0.65907606]\n",
      " [0.53458385 0.44168142 0.         0.39214783]\n",
      " [0.43728225 0.65907606 0.39214783 0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'soccer': 'tennis',\n",
       " 'tennis': 'badminton',\n",
       " 'rugby': 'soccer',\n",
       " 'badminton': 'tennis'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmat_300d_3 = get_glove_similarity_matrix(sample_sentences3, word_embeddings_300d, 300)\n",
    "closest_sentence(sample_sentences3, gmat_300d_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Similarity (Dissimilarity) of sports in different dimensional spaces?\n",
    "All 4 terms share the same general concept of `sport`. But there are so many different kinds of sports as we know - martial arts, field ball kicking, racket hitting etc.\n",
    "\n",
    "In both spaces, `tennis` is closer to `badminton` (racket hitting sports) as `rugby` is closer to `soccer` (field ball kicking sports). \n",
    "One interesting point found from above, in a lower dimensional where  `soccer` is said to be closer to `rugby`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T06:46:48.909946Z",
     "start_time": "2019-06-28T06:46:48.904675Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_sentences = [\"greek dish healthy\", \"malaysian food tasty\", \"movie comedy fun\", \"reading fictional entertaining\"]\n",
    "sample_sentences = [sentence.split(\" \") for sentence in sample_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T06:46:50.667148Z",
     "start_time": "2019-06-28T06:46:50.657858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.64845293 0.41033192 0.43772947]\n",
      " [0.64845293 0.         0.41957035 0.33179244]\n",
      " [0.41033192 0.41957035 0.         0.71958436]\n",
      " [0.43772947 0.33179244 0.71958436 0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'greek dish healthy': 'malaysian food tasty',\n",
       " 'malaysian food tasty': 'greek dish healthy',\n",
       " 'movie comedy fun': 'reading fictional entertaining',\n",
       " 'reading fictional entertaining': 'movie comedy fun'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmat_100d_1 = get_glove_similarity_matrix(sample_sentences, word_embeddings_100d, 100)\n",
    "print(gmat_100d_1)\n",
    "closest_sentence(sample_sentences, gmat_100d_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T06:49:26.217180Z",
     "start_time": "2019-06-28T06:49:26.207193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.50597631 0.23174479 0.25190822]\n",
      " [0.50597631 0.         0.24630075 0.18584596]\n",
      " [0.23174479 0.24630075 0.         0.5795299 ]\n",
      " [0.25190822 0.18584596 0.5795299  0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'greek dish healthy': 'malaysian food tasty',\n",
       " 'malaysian food tasty': 'greek dish healthy',\n",
       " 'movie comedy fun': 'reading fictional entertaining',\n",
       " 'reading fictional entertaining': 'movie comedy fun'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmat_300d_1 = get_glove_similarity_matrix(sample_sentences, word_embeddings_300d, 300)\n",
    "print(gmat_300d_1)\n",
    "closest_sentence(sample_sentences, gmat_300d_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T06:52:28.706831Z",
     "start_time": "2019-06-28T06:52:28.702603Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_sentences2 = [\"clean beautiful flat\", \"tidy comfortable apartment\", \"host responsive helpful\", \"guest great friendly\"]\n",
    "sample_sentences2 = [sentence.split(\" \") for sentence in sample_sentences2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T06:52:30.075496Z",
     "start_time": "2019-06-28T06:52:30.066465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.67390946 0.43533047 0.59942636]\n",
      " [0.67390946 0.         0.31802589 0.48821536]\n",
      " [0.43533047 0.31802589 0.         0.61105895]\n",
      " [0.59942636 0.48821536 0.61105895 0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clean beautiful flat': 'tidy comfortable apartment',\n",
       " 'tidy comfortable apartment': 'clean beautiful flat',\n",
       " 'host responsive helpful': 'guest great friendly',\n",
       " 'guest great friendly': 'host responsive helpful'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmat_100d = get_glove_similarity_matrix(sample_sentences2, word_embeddings_100d, 100)\n",
    "print(gmat_100d)\n",
    "closest_sentence(sample_sentences2, gmat_100d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T06:52:33.110534Z",
     "start_time": "2019-06-28T06:52:33.102171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.5292614  0.25007188 0.41155782]\n",
      " [0.5292614  0.         0.14884599 0.2918403 ]\n",
      " [0.25007188 0.14884599 0.         0.42941805]\n",
      " [0.41155782 0.2918403  0.42941805 0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clean beautiful flat': 'tidy comfortable apartment',\n",
       " 'tidy comfortable apartment': 'clean beautiful flat',\n",
       " 'host responsive helpful': 'guest great friendly',\n",
       " 'guest great friendly': 'host responsive helpful'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmat_300d = get_glove_similarity_matrix(sample_sentences2, word_embeddings_300d, 300)\n",
    "print(gmat_300d)\n",
    "closest_sentence(sample_sentences2, gmat_300d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
