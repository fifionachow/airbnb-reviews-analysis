{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T18:55:00.288964Z",
     "start_time": "2019-06-23T18:54:59.674422Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T17:01:40.373700Z",
     "start_time": "2019-06-23T17:01:40.370180Z"
    }
   },
   "source": [
    "# TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T22:22:06.061729Z",
     "start_time": "2019-06-23T22:21:47.363033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>comments_cleaned</th>\n",
       "      <th>comments_l</th>\n",
       "      <th>comments_token</th>\n",
       "      <th>comments_token_str</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>index</th>\n",
       "      <th>language</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>probability</th>\n",
       "      <th>review_length</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>362299</th>\n",
       "      <td>The host canceled this reservation 4 days befo...</td>\n",
       "      <td>host canceled reservation days arrival automat...</td>\n",
       "      <td>the host canceled this reservation 4 days befo...</td>\n",
       "      <td>[host, cancel, reserv, day, arriv, autom, post]</td>\n",
       "      <td>host cancel reserv day arriv autom post</td>\n",
       "      <td>2019-05-04</td>\n",
       "      <td>447965634</td>\n",
       "      <td>452636</td>\n",
       "      <td>en</td>\n",
       "      <td>34384353</td>\n",
       "      <td>0.958796</td>\n",
       "      <td>87</td>\n",
       "      <td>49082420</td>\n",
       "      <td>Maxime</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comments  \\\n",
       "362299  The host canceled this reservation 4 days befo...   \n",
       "\n",
       "                                         comments_cleaned  \\\n",
       "362299  host canceled reservation days arrival automat...   \n",
       "\n",
       "                                               comments_l  \\\n",
       "362299  the host canceled this reservation 4 days befo...   \n",
       "\n",
       "                                         comments_token  \\\n",
       "362299  [host, cancel, reserv, day, arriv, autom, post]   \n",
       "\n",
       "                             comments_token_str       date         id   index  \\\n",
       "362299  host cancel reserv day arriv autom post 2019-05-04  447965634  452636   \n",
       "\n",
       "       language  listing_id  probability  review_length  reviewer_id  \\\n",
       "362299       en    34384353     0.958796             87     49082420   \n",
       "\n",
       "       reviewer_name  sentence_length  \n",
       "362299        Maxime               14  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_reviews_df = pd.read_json(\"preprocessed_english_reviews.json\")\n",
    "eng_reviews_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T20:56:35.054927Z",
     "start_time": "2019-06-23T20:56:34.519319Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_reviews_df = eng_reviews_df[~eng_reviews_df.comments.str.contains(\"The host canceled this reservation\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T17:01:50.337845Z",
     "start_time": "2019-06-23T17:01:50.334198Z"
    }
   },
   "source": [
    "# Extraction Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tfidf\n",
    "\n",
    "https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T20:57:40.486264Z",
     "start_time": "2019-06-23T20:56:35.056961Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "eng_reviews_df['sentences'] = eng_reviews_df['comments'].apply(lambda x: sent_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T19:19:43.021022Z",
     "start_time": "2019-06-23T19:19:43.012410Z"
    }
   },
   "outputs": [],
   "source": [
    "from datajanitor.text import create_ngram, remove_stopword, symbols_replaced, decontracted, remove_html_tags, split_words_and_punctuation, apply_text_normalisation\n",
    "from utils import keep_token_pattern, chain\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "english_stop_words = stopwords.words('english')\n",
    "sb = SnowballStemmer(\"english\").stem\n",
    "\n",
    "operations = [{\"function\": symbols_replaced},\n",
    "              {\"function\": decontracted},\n",
    "              {\"function\": remove_html_tags},\n",
    "              {\"function\": split_words_and_punctuation},\n",
    "              {\"function\": remove_stopword, \"stopword_list\": english_stop_words},\n",
    "              {\"function\": keep_token_pattern, \"pattern\": '[a-zA-Z]{3,}'}\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T22:12:39.065087Z",
     "start_time": "2019-06-23T22:12:27.489321Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def dummy_fun(token):\n",
    "    return token\n",
    "\n",
    "count_vectorizer = TfidfVectorizer(\n",
    "                        strip_accents='unicode',\n",
    "                        preprocessor=dummy_fun,\n",
    "                        analyzer='word',\n",
    "                        ngram_range=(1, 1),\n",
    "                        min_df=10,\n",
    "                        use_idf=True, smooth_idf=True, \n",
    "                        max_features = 1000)\n",
    "\n",
    "bag_of_words = count_vectorizer.fit(eng_reviews_df['comments_token_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T21:06:12.115245Z",
     "start_time": "2019-06-23T20:58:05.062378Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_reviews_df['sentences_cleaned'] = eng_reviews_df['sentences'].apply(lambda x: [chain(sentence.lower(), operations) for sentence in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T22:00:48.242500Z",
     "start_time": "2019-06-23T22:00:43.849352Z"
    }
   },
   "outputs": [],
   "source": [
    "grouped_eng_reviews = eng_reviews_df.groupby('listing_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T22:20:38.274090Z",
     "start_time": "2019-06-23T22:20:37.545683Z"
    }
   },
   "outputs": [],
   "source": [
    "listing_2818 = grouped_eng_reviews.get_group(2818)\n",
    "listing_2818_review = np.array([review_sentence for review_sentences in listing_2818['sentences_cleaned'].tolist() for review_sentence in review_sentences])\n",
    "listing_2818_review_weights = [count_vectorizer.transform(sentence).sum(axis=0) if sentence else np.array(0) for sentence in listing_2818_review]\n",
    "listing_2818_sentence_weights = np.array([w.sum() for w in listing_2818_review_weights])\n",
    "top_3_sentences_pos_listing_2818 = listing_2818_sentence_weights.argsort()[-3:][::-1]\n",
    "top_3_listing_2818_sentences = np.array(listing_2818_review)[top_3_sentences_pos_listing_2818]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T22:21:06.976686Z",
     "start_time": "2019-06-23T22:21:06.970315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['room comfortable colourful light quiet equipped everything could possibly need daniel flat spotless beautifully furnished great location daniel perfect host spending first hour arrival talking amsterdam answering many questions showing get around',\n",
       " 'also nicely laid tea coffee set room different tour books amsterdam helped plan days plastic map could carry throughout adventures bicycle available rent matching set shampoo conditioner lotion bathroom etc adapter plugs',\n",
       " 'daniel supplied electric water heater room could consume tea coffee leisure added laundry showed use hidden airbnb map real time supplied lanyard phone number never used reassuring']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\" \".join(top_sentence) for top_sentence in top_3_listing_2818_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T22:27:50.822785Z",
     "start_time": "2019-06-23T22:27:49.132309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nice room negative thing roof light made hard sleep wasnt dark witch two younger children hot sun came really small bathroom hard help kids shower small shower',\n",
       " 'loved stay shawna flat lightfull better put eyes cover clean website hidden airbnb central next grocery shops kind website hidden airbnb nice family let make reservation van gogh museum make cue',\n",
       " 'friendly host family amazing appartement amsterdam next visit would definitely like live supermarket next door open lots pubs caf restaurants cash machine min walk leidsplein aawww airport wifi ofcourse stereo equipment left headphone connector cable plug ipod iphone whatever']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listing_df = grouped_eng_reviews.get_group(82482)\n",
    "get_listing_summary(listing_df, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-23T22:26:50.693126Z",
     "start_time": "2019-06-23T22:26:50.662667Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_listing_summary(listing_df, count_vectorizer, top_n_sentence=3):\n",
    "    \"\"\"\n",
    "    Importantly, to ensure long sentences do not have unnecessarily high scores over short sentences, we divided each score of a sentence by the number of words found in that sentence.\n",
    "    \"\"\"\n",
    "    listing_review = np.array([review_sentence for review_sentences in listing_df['sentences_cleaned'].tolist() for review_sentence in review_sentences])\n",
    "    listing_review_weights = [count_vectorizer.transform(sentence).sum(axis=0) if sentence else np.array(0) for sentence in listing_review]\n",
    "    listing_sentence_weights = np.array([w.sum() for w in listing_review_weights])\n",
    "    top_3_sentences_pos_listing = listing_sentence_weights.argsort()[-1*top_n_sentence:][::-1]\n",
    "    top_3_listing_sentences = [\" \".join(sentence) for sentence in np.array(listing_review)[top_3_sentences_pos_listing]]\n",
    "    return top_3_listing_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_average_score(sentence_weight) -> int:\n",
    "   \n",
    "    # Calculating the average score for the sentences\n",
    "    sum_values = 0\n",
    "    for entry in sentence_weight:\n",
    "        sum_values += sentence_weight[entry]\n",
    "\n",
    "    # Getting sentence average value from source text\n",
    "    average_score = (sum_values / len(sentence_weight))\n",
    "\n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstraction Method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
